/*
========================================================================================
    nf-core/eqtl Nextflow base config file
========================================================================================
    A 'blank slate' config file, appropriate for general use on most high performance
    compute environments. Assumes that all software is installed and available on
    the PATH. Runs in `local` mode - all jobs will be run on the logged in environment.
----------------------------------------------------------------------------------------
*/
params{
    chunkSize=20 // For SAIGE and LIMIX how many genes to chunk at a time to run on a single computation node.
    pre_aggregated_counts_folder = '' //this folder should contain subfolders with ___genotype_phenotype_mapping.tsv and ___phenotype_file.tsv, it is useful so that the aggrgations and normalisations do not need to be performed multiple times with each pipeline rerun. Typically results stored in .results/aggregated_counts
    gtf_gene_identifier = 'gene_id' // in your gtf file what colname do you have for the genes - can be gene_id or gene_name dependant of gtf file used. 
    chromosomes_to_test='' // Which chromosomes to test? If you set this to '' then it will analyse all available
    //chromosomes_to_test=[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,'X','Y'] // !! Important that you plink2_filters and bcftools_filters  bellow align with this especially if you need X then you also need to provide a path to sex info for plink2 conversion
    
    LIMIX{
        run=false // Are we running limix? 
        callRate=0.95 // what call rate to use? 
        blockSize=1500 // What block size to use for mapping?
        hwe=0.0000001 // What hwe to use? 
        numberOfPermutations=10 // how many permutations to use?
    }

    SAIGE{
        run=true // Are we running saige? 
        nr_expression_pcs=5 // How many scRNA expression PCs to use?
        q_val_threshold_for_conditioning=0.05 // if there are more than 1 value with a qvalue lover than this for gene then we send it in for conditioning rounds.
        minMAF=0.05 // what minMAF to use in TensorQTL?
        minMAC=20 // what Min MAC to use? 
        SPAcutoff=2 // what SPA cuttof to use?
        markers_per_chunk=10000 // how many markers per chunk to test in tensorqtl?
        covariate_obs_columns='' // What andata.obs to use as covariates in SAIGE
        cis_trans_mode = 'cis' // cis|trans -- whether to run in cis or trans mode. If running in cis mode pipeline will use window size to test for associations in the +- surounding window of the TSS.
        trans_chr_to_test='SAME' // if you put SAME it will test trans on the chromosome the gene is on. Otherwise specify chr as in 2 or 20 etc
        aggregation_subentry = '' // do we want to run only a few celltypes instead of all? 
        relatednessCutoff = 0.05 // What relatedness cutoff to use for Saige QTL.
        step1_extra_flags = '' // extra flags to provide in S1 model fitting
    }

    TensorQTL{
        run=true // Are we running TensorQTL?
        optimise_pcs = true // Whether to pick the most optimal PCs to use for the downstram analysis. 
        interaction_file='' // to run interaction, provide a TSV with genotype ID and interaction 
        interaction_maf = 0.01 // what interaction MAF to use in tensorqtl interactions test
        interaction_pc_cor_threshold = 0.25 // Drop PCs correlated with interaction above this threshold. Use 1 if you don not want to drop PCs
        interaction_gsea = false // Run GSEA on the interaction terms
        trans_by_cis=true // Run trans-by-cis analysis (all genes, limiting variants) following OPTIM PCs?
        trans_by_cis_variant_list='' // Provide a tsv with variant_id and condition_name OR leave empty to use all lead variants from eGenes
        trans_of_cis=true // Run trans-of-cis eQTL analysis (all variants, limiting genes), following OPTIM PCs?
        trans_by_cis_pval_threshold = 0.01 // when running trans by cis what pval threshold to use.
        alpha=0.05  ///What Alpha value to use in tensorqtl
        chrom_to_map_trans = '' // Option to run GWAS trans analysis - i.e if you specify 2 in here Tensorqtl will run all genes acros genome against all the SNPs on the chromosome 2.
        aggregation_subentry = '' // Are we running all celltypes or a subset of celltypes available in h5ad file?
        map_independent_qtls = false
    }

    JAXQTL{
        run=true // Are we running JaxQTL?
        number_of_genes_per_chunk = 50 // How many genes to run at a time for the JaxQTL chunk.
        use_gpu = false // should we use cpus or GPU for the jobs?
    }

    use_sample_pca = true // Set to false if gene PCA is needed
    
    covariates{
        nr_phenotype_pcs = '2,4' // this is used for Tensorqtl and Limix
        nr_genotype_pcs = 4 // How many genotype PCs to use
        genotype_pc_filters = '--indep-pairwise 50 5 0.2'
        extra_covariates_file = '' // Specify any additional covariates to include in the analysis here.
        // These will be used alongside default covariates during modeling or QC.
        // Format should match: sample IDs as column headers, covariate names as row labels.
        // sample   682_683  683_684  684_685  685_686  686_687  687_688  688_689  689_690  690_691  691_692  692_693  693_694
        // cov1          1        2        0        2        2        3        2        1        0        0        0        1
        
        genotype_pcs_file = '' //if you have already precalculated genotype PCs you can provide it in here. Make sure that the number of genotype PCs above is covered (for exaple that there are at least 4 PCs represented since we defined that GPcs to use == 4)
    }

    genotypes{
        subset_genotypes_to_available=false // if true, then the expression data will be first processed and then the samples availbale in all the expression data will be subset from genotype files to avoid loading large genotype plink2_filters
        apply_bcftools_filters=true // if true and vcf file is provided then preprocessing will be done on the files. Whether we want to perform the folowing filters as per bcftools_filters
        use_gt_dosage = true   // Set true if using dosages (e.g., from VCF DS field or PGEN format with .pvar/.psam).
        preprocessed_bed_file=''     // For traditional PLINK BED format input. Again, leave input_vcf empty.
        preprocessed_pgen_file=''     // Provide a path to .pgen/.psam/.pvar trio here if already converted. When used, leave input_vcf empty.
        preprocessed_bgen_file ='' // if user has already provided bgen file for Limix conversion will be avoided and this file will be used instead
        // Note: FID_IID is usually combined with an underscore, but PLINK2 typically uses IID only for downstream matching.
    }

    outdir='results' // where to output results
    copy_mode = "rellink" // method of processing files in work dir.
    split_aggregation_adata=false  // Should we split the andata per condition before proceeding with the agregations and normalisations
    genotype_phenotype_mapping_file = '' 
    // TSV with three columns: phenotype_id, genotype_id, Sample_Category.
    // Sample_Category can be used to split the analysis (e.g. for bulk or different stimulations); if not needed, just use a single label for all samples (e.g. "default").
    aggregation_columns='Azimuth:predicted.celltype.l1' //for the scrna h5ad file define which annotations to use when aggregating the data. Can be one value or multiple comma separated values
    aggregation_subentry='' // 'Mono,B,DC' If provided only these subcolumns that are contained within the aggregation_columns entry will be analysed.
    sample_column='Pool.Donor' // for the scrna h5ad defines which column has the sample name (can be identical to gt_id_column if sample=individual)
    utilise_gpu = false // whether to use gpu in tensorqtl or not. 
    gtf_type='gene' //# 'gene|transcript' - if we are using default gtf file as an input we need to know if we are looking at the transcripts of genes. used when gtf file is read to select necessary genes.
    input_tables_column_delimiter = '\t' // by default pipeline takes tsv files but csv could be provided too if this is changed.
    n_min_cells = '5' // The number of min cells for individual to select it for use in qtl mapping. 
    n_min_individ = '30' //Number of individuals that has this particular celltype or aggregation column. Do not select less than 25 since this may result in a permutation issue with tensorqtl
    percent_of_population_expressed=0.2 // whats the proportion of individuals that has to have the value !=0, Please do not go below 2% as this will result in a lot of low expressed genes which will cause issues in SAIGE and TensorQTL
    cell_percentage_threshold = 0 // % of cells that must express the gene
    position = 'TSS' // [ TSS|midpoint ]this can be TSS (for transcriptome, splicing and rna seq experiments) or midpoint (typically for Chi or ATAC qtls)
    aggregation_method = 'dMean,dSum' // can be: dMean|dSum or both dMean,dSum separated by comma
    inverse_normal_transform = 'FALSE' // Inverse normal trasnform data as part of normalisation
    bcftools_filters = '--max-alleles 2 -m2 -M2 -v snps' // Filters to apply to input vcf file if genotypes.apply_bcftools_filters = true
    plink2_filters = '--allow-extra-chr 0 --chr 1-22 X Y XY --snps-only --rm-dup exclude-all'  // please do not provide --output-chr chrM  as we need chr prefix not to be present in pipeline for bed and pgen files. Filters to apply to vcf to bed conversion file if genotypes.apply_bcftools_filters = true
    //# for plink2 filters you also need to provide --update-sex /path/to/sex/info/file/file.txt which contains all the sample info of the sex in tsv format - FID   IID   SEX
    maf = 0.01 // MAF to use in the qtl mapping
    hwe= 0.0000001 // hwe to use in qtl mapping
    windowSize=1000000 // Window around TSS of the gene +-
    numberOfPermutations=1000 // how many permutation to use
    filter_method = 'None' // filterByExpr|HVG|None  // filter genes to test based on these methods.
    norm_method = 'DESEQ'  //'DESEQ|TMM|NONE' // what normalisation method to use for bulk datasets
    tmpdir = "${launchDir}/work" // where to store the temp files.

    dMean_norm_method = 'cp10k' //  'cp10k'|'scT'|'pf_log1p_pf'|'NONE' Normlisation method for dMean, can be cp10k or a precomputed normalised counts layer in the anndata file
    normalise_before_or_after_aggregation = 'after' // before|after - decission whether to normalise the full dataframe before splitting them in subcategories, or after, i.e normalising per subcategory independently.

    // CONTAINERS
    eqtl_container = 'https://yascp.cog.sanger.ac.uk/public/singularity_images/eqtl_29_11_2024.sif'
    eqtl_docker='mercury/eqtl:29_11_2024'

    saige_container = 'https://yascp.cog.sanger.ac.uk/public/singularity_images/saigeeqtl_0_1_0.sif'
    saige_docker='mercury/saige_eqtl:10_07_2025'

    saige_grm_container = 'https://yascp.cog.sanger.ac.uk/public/singularity_images/saige_grm_dec2024.sif'
    saige_grm_docker = 'mercury/saige_grm:14_07_2025'
    
    limix_container='https://yascp.cog.sanger.ac.uk/public/singularity_images/limix_2024_25_11.sif'
    limix_docker='mercury/limix_eqtl:25_07_2024'
    
}


process {
    cache = 'lenient'
    maxRetries    = 5
    maxErrors     = '-1'
    errorStrategy = 'retry'
    time   = { 3.h   * task.attempt    }

    withLabel:process_high_memory {
        memory = { 80.GB * task.attempt  }
    }
    withLabel:process_medium_memory {
        memory = { 60.GB * task.attempt }
    }

    withLabel:process_tiny {
      cpus = 1
      maxRetries    = 2
      memory = 1.GB
      time   = {  2.h   * task.attempt }
    }

    withLabel:process_low {
        cpus   = { 1     * task.attempt    }
        memory = { 10.GB * task.attempt }
        time   = { 3.h   * task.attempt   }
    }
    withLabel:process_medium {
        cpus   = { 1     * task.attempt   }
        memory = { 30.GB * task.attempt  }
        time   = { 6.h   * task.attempt    }
    }
    withLabel:process_high {
        cpus   = { 1    * task.attempt    }
        memory = { 20.GB * task.attempt }
        time   = { 16.h  * task.attempt   }
    }

    withLabel:error_ignore {
        errorStrategy = 'ignore'
    }
    withLabel:error_retry {
        errorStrategy = 'retry'
        maxRetries    = 2
    }
    withName: LIMIX{
        errorStrategy = { task.attempt <= 2 ? 'retry' : 'ignore' }
        memory = { 20.GB * task.attempt }
    }    
    withName: SAIGE_S1{
        time   = { 12.h   * task.attempt    }
        maxRetries    = 3
        memory = { 10.GB * task.attempt }
    }
    withName: SAIGE_QVAL_COR{
        time   = { 12.h   * task.attempt    }

    }

    withName: DETERMINE_TSS_AND_TEST_REGIONS{
        memory = { 5.GB * task.attempt }
        
    }

    withName: PREPROCESS_GENOTYPES{
        memory = { 8.GB * task.attempt }
        cpus  = 1 
        
    }
    withName: GENOTYPE_PC_CALCULATION{
        cpus  = 1 
        memory = { 10.GB * task.attempt }
    }

    withName: AGGREGATE_UMI_COUNTS{
        cpus  = 1 
        
    }
    withName: SAIGE_S2_CIS{
        cpus  = 1 
        memory = { 4.GB * task.attempt }
    }
    
    withName: PHENOTYPE_PCs{
        cpus  = 1 
        time   = { 12.h   * task.attempt    }
        memory = { 16.GB * task.attempt } 
    }
    
    withName: SUBSET_PCS{
        cpus  = 1 
        memory = { 2.GB * task.attempt }
        
    }
    withName: NORMALISE_and_PCA_PHENOTYPE{
        cpus  = 1 
        memory = { 10.GB * task.attempt }
    }
    withName: PGEN_CONVERT{
        cpus  = 1 
        memory = { 10.GB * task.attempt }
    }
    withName: PREPERE_EXP_BED{
        cpus  = 1 
        memory = { 15.GB * task.attempt }
    }
    
    withName: SAIGE_S3{
        time   = { 12.h   * task.attempt    }
    }    
    withName: AGGREGATE_QTL_ALLVARS{
        time   = { 12.h   * task.attempt    }
        memory = { 15.GB * task.attempt }
    }
    withName:SPLIT_AGGREGATION_ADATA {
        memory = { 64.GB * task.attempt }
    }
}

singularity {
    enabled = true
    autoMounts = true
    runOptions = '--nv'
}