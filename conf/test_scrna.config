/*
========================================================================================
    Nextflow config file for running full-size tests
========================================================================================
    Defines input files and everything required to run a full size pipeline test.

    Use as follows:
        nextflow run nf-core/eqtl -profile test_full,<docker/singularity>

----------------------------------------------------------------------------------------
*/

params {

    method= 'single_cell' //or a [bulk | single_cell] (if single cell used the *phenotype_file* is a h5ad file)
    input_vcf ='https://yascp.cog.sanger.ac.uk/public/test_datasets/full_test_dataset/smaller_dataset/genotypes/coding_chr_Pool1_INFO_0.9_MAF_0.001.vcf.gz'
    genotype_phenotype_mapping_file = 'https://yascp.cog.sanger.ac.uk/public/test_datasets/QTLight/gt_mapping_test.tsv' // annotation file containing Genotype | RNA and | Condiion
    annotation_file = 'https://yascp.cog.sanger.ac.uk/public/test_datasets/QTLight/Homo_sapiens.GRCh38.99.gtf' //assets file that has start and end positions for the genes, this one is using hg38
    phenotype_file = 'https://yascp.cog.sanger.ac.uk/public/test_datasets/QTLight/test_Onek1kPool1.h5ad' //this should point to h5ad file in a single cell experiments, ensuring adata.X is raw counts
    aggregation_columns='Azimuth:predicted.celltype.l1' //for the scrna h5ad file define which annotations to use when aggregating the data. Can be one value or multiple comma separated values
    gt_id_column='Pool.Donor' //for the scrna h5ad defines which column has individual level id (corresponding to the VCF file)
    sample_column='Pool.Donor' // for the scrna h5ad defines which column has the sample name (can be identical to gt_id_column if sample=individual)
    n_min_individ = '3'
    split_aggregation_adata=true
    windowSize=250000
    aggregation_subentry='Mono'

    LIMIX{
        run=true // Are we running limix? 
        callRate=0.95 // what call rate to use? 
        blockSize=1500 // What block size to use for mapping?
        hwe=0.0000001 // What hwe to use? 
        numberOfPermutations=10 // how many permutations to use?
        chromosomes_to_test=[18,1] // Which chromosomes to test?
    }

    covariates{
        nr_phenotype_pcs = '2,4' // this is used for Tensorqtl and Limix
        nr_genotype_pcs = 4 // How many genotype PCs to use
        genotype_pc_filters = '--indep-pairwise 50 5 0.2 --bad-ld'
        extra_covariates_file = '' // if there is any extra covariates that you would like to use, provide it here.
        genotype_pcs_file = '' //if you have already precalculated genotype PCs you can provide it in here. Make sure that the number of genotype PCs above is covered (for exaple that there are at least 4 PCs represented since we defined that GPcs to use == 4)
    }


}

process{
    withName: PHENOTYPE_PCs{
        cpus  = 1 
        time   = { 12.h   * task.attempt    } 
         memory = { 1.GB * task.attempt }
    }
    withName: SAIGE_S1{
        time   = { 12.h   * task.attempt    }
        maxRetries    = 3
        maxForks = 6
        memory = { 12.GB * task.attempt }
    }

    withName: LIMIX{
        time   = { 12.h   * task.attempt    }
        maxRetries    = 3
        maxForks = 8
        memory = { 12.GB * task.attempt }
    }
    
    withName: DETERMINE_TSS_AND_TEST_REGIONS{
        time   = { 12.h   * task.attempt    }
        maxRetries    = 3
        maxForks = 3
        memory = { 4.GB * task.attempt }
    }
    withName: SAIGE_S2_CIS{
        time   = { 12.h   * task.attempt    }
        maxRetries    = 3
        maxForks = 3
        memory = { 4.GB * task.attempt }
    }
    
    withName: SPLIT_AGGREGATION_ADATA{
        time   = { 12.h   * task.attempt    }
        maxRetries    = 3
        memory = { 12.GB * task.attempt }
    }    

}